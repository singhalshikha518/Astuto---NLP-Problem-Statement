{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a8b3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f450bf5deb8340338e275716078a65d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-09 19:40:37 INFO: Downloading default packages for language: en (English)...\n",
      "2022-01-09 19:40:38 INFO: File exists: /home/ec2-user/stanza_resources/en/default.zip.\n",
      "2022-01-09 19:40:44 INFO: Finished downloading models and saved to /home/ec2-user/stanza_resources.\n",
      "2022-01-09 19:40:44 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-01-09 19:40:44 INFO: Use device: gpu\n",
      "2022-01-09 19:40:44 INFO: Loading: tokenize\n",
      "2022-01-09 19:40:44 INFO: Loading: pos\n",
      "2022-01-09 19:40:44 INFO: Loading: lemma\n",
      "2022-01-09 19:40:44 INFO: Loading: depparse\n",
      "2022-01-09 19:40:45 INFO: Loading: sentiment\n",
      "2022-01-09 19:40:46 INFO: Loading: constituency\n",
      "2022-01-09 19:40:46 INFO: Loading: ner\n",
      "2022-01-09 19:40:47 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['product', []], ['dayexperience', ['1st', 'sharing']], ['upgrade', ['this', 'major']], ['iphone', []], ['iphone', ['smaller']], ['iphone', ['smaller']], ['vsperformance', [')']], ['performanceratio', []], ['camera', ['fantastic']], ['batterylife', ['better']], ['phone', ['light']], ['phone', []], ['model', ['this', 'hold']], ['opinion', []]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import stanza\n",
    "stanza.download('en')\n",
    "\n",
    "\n",
    "# Make sure you have downloaded the StanfordNLP English model and other essential tools using,\n",
    "# stanfordnlp.download('en')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def aspect_sentiment_analysis(txt, stop_words, nlp):\n",
    "    \n",
    "    txt = txt.lower() # LowerCasing the given Text\n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    fcluster = []\n",
    "    totalfeatureList = []\n",
    "    finalcluster = []\n",
    "    dic = {}\n",
    "\n",
    "    for line in sentList:\n",
    "        newtaggedList = []\n",
    "        txt_list = nltk.word_tokenize(line) # Splitting up into words\n",
    "        taggedList = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "        newwordList = []\n",
    "        flag = 0\n",
    "        for i in range(0,len(taggedList)-1):\n",
    "            if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"): # If two consecutive words are Nouns then they are joined together\n",
    "                newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "                flag=1\n",
    "            else:\n",
    "                if(flag==1):\n",
    "                    flag=0\n",
    "                    continue\n",
    "                newwordList.append(taggedList[i][0])\n",
    "                if(i==len(taggedList)-2):\n",
    "                    newwordList.append(taggedList[i+1][0])\n",
    "\n",
    "        finaltxt = ' '.join(word for word in newwordList) \n",
    "        new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "        wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "        #print('words==',wordsList)\n",
    "        taggedList = nltk.pos_tag(wordsList)\n",
    "        \n",
    "        doc = nlp(finaltxt)\n",
    "        dep_node = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "        \n",
    "        featureList = []\n",
    "        categories = []\n",
    "        #print('list',taggedList)\n",
    "        for i in taggedList:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i)) # For features for each sentence\n",
    "                totalfeatureList.append(list(i)) # Stores the features of all the sentences in the text\n",
    "                categories.append(i[0])\n",
    "\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "            \n",
    "    for i in totalfeatureList:\n",
    "        dic[i[0]] = i[1]\n",
    "    \n",
    "    for i in fcluster:\n",
    "        if(dic[i[0]]==\"NN\"):\n",
    "            finalcluster.append(i)\n",
    "        \n",
    "    return(finalcluster)\n",
    "\n",
    "nlp = stanza.Pipeline('en') \n",
    "stop_words = set(stopwords.words('english'))\n",
    "txt = \"There are a bunch of reviews on the product already. Just sharing my 1st day experience so far, I loved it. This is a major upgrade for me, I jumped from iPhone 7 to 13mini after 4 years. Always wanted to smaller iPhone and looks like so far 13mini is by far the best predecessor of iPhone 7( for me interns of size vs performance ratio). Fantastic Camera, better battery life, and incredibly light phone. All the faults from iPhone 12mini are addressed in this phone. This will be a model which I can hold for at-least next three years in my opinion.\"\n",
    "print(aspect_sentiment_analysis(txt, stop_words, nlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b520a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
